{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromedriver-autoinstaller in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (0.6.4)\n",
      "Requirement already satisfied: packaging>=23.1 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from chromedriver-autoinstaller) (24.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: loguru in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from loguru) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from loguru) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: pymongo in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from pymongo) (2.7.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (0.3.14)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (0.3.14)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (0.3.29)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (0.2.10)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.24.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (0.3.5)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain<0.4.0,>=0.3.14->langchain-community) (2.8.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.29->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain-community) (2.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
      "Collecting poetry\n",
      "  Downloading poetry-2.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting build<2.0.0,>=1.2.1 (from poetry)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting cachecontrol<0.15.0,>=0.14.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Downloading cachecontrol-0.14.2-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n",
      "  Using cached cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dulwich<0.23.0,>=0.22.6 (from poetry)\n",
      "  Downloading dulwich-0.22.7-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting fastjsonschema<3.0.0,>=2.18.0 (from poetry)\n",
      "  Downloading fastjsonschema-2.21.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting installer<0.8.0,>=0.7.0 (from poetry)\n",
      "  Using cached installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
      "Collecting keyring<26.0.0,>=25.1.0 (from poetry)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=24.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from poetry) (24.2)\n",
      "Collecting pkginfo<2.0,>=1.12 (from poetry)\n",
      "  Using cached pkginfo-1.12.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.0.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from poetry) (4.3.6)\n",
      "Collecting poetry-core==2.0.0 (from poetry)\n",
      "  Downloading poetry_core-2.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pyproject-hooks<2.0.0,>=1.0.0 (from poetry)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: requests<3.0,>=2.26 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from poetry) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from poetry) (1.0.0)\n",
      "Collecting shellingham<2.0,>=1.5 (from poetry)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tomlkit<1.0.0,>=0.11.4 (from poetry)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting trove-classifiers>=2022.5.19 (from poetry)\n",
      "  Downloading trove_classifiers-2025.1.7.14-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting virtualenv<21.0.0,>=20.26.6 (from poetry)\n",
      "  Using cached virtualenv-20.28.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from build<2.0.0,>=1.2.1->poetry) (0.4.6)\n",
      "Collecting msgpack<2.0.0,>=0.5.2 (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Using cached msgpack-1.1.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting filelock>=3.8.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting crashtest<0.5.0,>=0.4.1 (from cleo<3.0.0,>=2.1.0->poetry)\n",
      "  Using cached crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n",
      "  Downloading rapidfuzz-3.11.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from dulwich<0.23.0,>=0.22.6->poetry) (2.3.0)\n",
      "Collecting pywin32-ctypes>=0.2.0 (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Using cached pywin32_ctypes-0.2.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting importlib_metadata>=4.11.4 (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting jaraco.classes (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Using cached jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asrah\\desktop\\llm_twin\\myenv\\lib\\site-packages (from requests<3.0,>=2.26->poetry) (2024.12.14)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.26.6->poetry)\n",
      "  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata>=4.11.4->keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting more-itertools (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring<26.0.0,>=25.1.0->poetry)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Downloading poetry-2.0.0-py3-none-any.whl (253 kB)\n",
      "Downloading poetry_core-2.0.0-py3-none-any.whl (544 kB)\n",
      "   ---------------------------------------- 0.0/544.1 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 262.1/544.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 544.1/544.1 kB 1.1 MB/s eta 0:00:00\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading cachecontrol-0.14.2-py3-none-any.whl (21 kB)\n",
      "Using cached cleo-2.1.0-py3-none-any.whl (78 kB)\n",
      "Downloading dulwich-0.22.7-cp311-cp311-win_amd64.whl (599 kB)\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 599.1/599.1 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading fastjsonschema-2.21.1-py3-none-any.whl (23 kB)\n",
      "Using cached installer-0.7.0-py3-none-any.whl (453 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Using cached pkginfo-1.12.0-py3-none-any.whl (32 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading trove_classifiers-2025.1.7.14-py3-none-any.whl (13 kB)\n",
      "Using cached virtualenv-20.28.1-py3-none-any.whl (4.3 MB)\n",
      "Using cached crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "Using cached msgpack-1.1.0-cp311-cp311-win_amd64.whl (74 kB)\n",
      "Using cached pywin32_ctypes-0.2.3-py3-none-any.whl (30 kB)\n",
      "Downloading rapidfuzz-3.11.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.8/1.6 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.6/1.6 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 1.6 MB/s eta 0:00:00\n",
      "Using cached jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Installing collected packages: trove-classifiers, fastjsonschema, distlib, zipp, tomlkit, shellingham, rapidfuzz, pywin32-ctypes, pyproject-hooks, poetry-core, pkginfo, msgpack, more-itertools, installer, filelock, dulwich, crashtest, backports.tarfile, virtualenv, jaraco.functools, jaraco.context, jaraco.classes, importlib_metadata, cleo, cachecontrol, build, keyring, poetry\n",
      "Successfully installed backports.tarfile-1.2.0 build-1.2.2.post1 cachecontrol-0.14.2 cleo-2.1.0 crashtest-0.4.1 distlib-0.3.9 dulwich-0.22.7 fastjsonschema-2.21.1 filelock-3.16.1 importlib_metadata-8.5.0 installer-0.7.0 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.1.0 keyring-25.6.0 more-itertools-10.5.0 msgpack-1.1.0 pkginfo-1.12.0 poetry-2.0.0 poetry-core-2.0.0 pyproject-hooks-1.2.0 pywin32-ctypes-0.2.3 rapidfuzz-3.11.0 shellingham-1.5.4 tomlkit-0.13.2 trove-classifiers-2025.1.7.14 virtualenv-20.28.1 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "! pip install chromedriver-autoinstaller\n",
    "! pip install selenium\n",
    "! pip install loguru\n",
    "! pip install tqdm\n",
    "! pip install beautifulsoup4\n",
    "! pip install pymongo\n",
    "! pip install langchain-community \n",
    "! pip install poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NoSQLBaseDocument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUserDocument\u001b[39;00m(\u001b[43mNoSQLBaseDocument\u001b[49m):\n\u001b[0;32m      2\u001b[0m     first_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m      3\u001b[0m     last_name: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NoSQLBaseDocument' is not defined"
     ]
    }
   ],
   "source": [
    "class UserDocument(NoSQLBaseDocument):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "    class Settings:\n",
    "        name = \"users\"\n",
    "\n",
    "    @property\n",
    "    def full_name(self):\n",
    "        return f\"{self.first_name} {self.last_name}\"\n",
    "\n",
    "\n",
    "class Document(NoSQLBaseDocument, ABC):\n",
    "    content: dict\n",
    "    platform: str\n",
    "    author_id: UUID4 = Field(alias=\"author_id\")\n",
    "    author_full_name: str = Field(alias=\"author_full_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class DataCategory(StrEnum):\n",
    "    PROMPT = \"prompt\"\n",
    "    QUERIES = \"queries\"\n",
    "\n",
    "    INSTRUCT_DATASET_SAMPLES = \"instruct_dataset_samples\"\n",
    "    INSTRUCT_DATASET = \"instruct_dataset\"\n",
    "    PREFERENCE_DATASET_SAMPLES = \"preference_dataset_samples\"\n",
    "    PREFERENCE_DATASET = \"preference_dataset\"\n",
    "\n",
    "    POSTS = \"posts\"\n",
    "    ARTICLES = \"articles\"\n",
    "    REPOSITORIES = \"repositories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Chromium executable not found at '/usr/bin/chromium-browser'.\n",
      "Using default ChromeDriver configuration. This might lead to errors.\n",
      "Starting to scrape...\n",
      "No more content to load.\n",
      "Scraped 434 text posts.\n",
      "Data saved to 'tumblr_text_posts.json'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "BASE_URL = \"https://cassassinate123.tumblr.com\"\n",
    "# Define the base Tumblr URL\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initialize the Selenium WebDriver.\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    chromium_path = '/usr/bin/chromium-browser' \n",
    "    if os.path.exists(chromium_path):\n",
    "        chrome_options.binary_location = chromium_path\n",
    "    else:\n",
    "        print(f\"Warning: Chromium executable not found at '{chromium_path}'.\")\n",
    "        print(\"Using default ChromeDriver configuration. This might lead to errors.\")\n",
    "       \n",
    "    service =  Service(\"C:/Users/asrah/AppData/Local/Programs/Python/Launcher/chromedriver-win64/chromedriver.exe\" )\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def scroll_and_collect(driver, max_scrolls=5000, scroll_pause=2):\n",
    "    \"\"\"Scroll down and collect all text from <p> tags.\"\"\"\n",
    "    all_posts = []\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(max_scrolls):\n",
    "        # Scroll down to the bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        posts = [p.get_text(strip=True) for p in soup.find_all(\"p\") if p.get_text(strip=True)]\n",
    "        all_posts.extend(posts)\n",
    "\n",
    "        # Check if the page height stops increasing (end of scroll)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"No more content to load.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    return list(set(all_posts))  # Remove duplicates\n",
    "\n",
    "def main():\n",
    "    driver = initialize_driver()\n",
    "    driver.get(BASE_URL)\n",
    "\n",
    "    try:\n",
    "        print(\"Starting to scrape...\")\n",
    "        all_posts = scroll_and_collect(driver, max_scrolls=10000, scroll_pause=2)  # Adjust scrolls/pause as needed\n",
    "        print(f\"Scraped {len(all_posts)} text posts.\")\n",
    "\n",
    "        # Save the data to a JSON file\n",
    "        with open(\"tumblr_text_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_posts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(\"Data saved to 'tumblr_text_posts.json'.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_and_collect(driver, scroll_pause=1):\n",
    "    \"\"\"Scroll down and collect all text from <p> tags until no new content is loaded.\"\"\"\n",
    "    all_posts = set()\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause)  # Pause to allow content to load\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        posts = {p.get_text(strip=True) for p in soup.find_all(\"p\") if p.get_text(strip=True)}\n",
    "        all_posts.update(posts)\n",
    "\n",
    "        # Check if the page height stops increasing (end of scroll)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            print(\"No more content to load.\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    return list(all_posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Chromium executable not found at '/usr/bin/chromium-browser'.\n",
      "Using default ChromeDriver configuration. This might lead to errors.\n",
      "No more content to load.\n",
      "Data saved to 'tumblr_text_posts.json'.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    driver = initialize_driver()\n",
    "    if driver is None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        driver.get(BASE_URL)\n",
    "        all_posts = scroll_and_collect(driver)\n",
    "        with open(\"tumblr_text_posts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(all_posts, f, ensure_ascii=False, indent=4)\n",
    "        print(\"Data saved to 'tumblr_text_posts.json'.\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poetry (version 2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!poetry --version  # Should show Poetry version 1.8.3 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using virtualenv: C:\\Users\\asrah\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\Local\\pypoetry\\Cache\\virtualenvs\\llm-engineering-_sOKo1qk-py3.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating virtualenv llm-engineering-_sOKo1qk-py3.11 in C:\\Users\\asrah\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\n"
     ]
    }
   ],
   "source": [
    "!poetry env use 3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "Package operations: 175 installs, 0 updates, 0 removals\n",
      "\n",
      "  - Installing certifi (2024.12.14)\n",
      "  - Installing charset-normalizer (3.4.1)\n",
      "  - Installing h11 (0.14.0)\n",
      "  - Installing idna (3.10)\n",
      "  - Installing sniffio (1.3.1)\n",
      "  - Installing typing-extensions (4.12.2)\n",
      "  - Installing urllib3 (2.3.0)\n",
      "  - Installing annotated-types (0.7.0)\n",
      "  - Installing anyio (4.8.0)\n",
      "  - Installing httpcore (1.0.7)\n",
      "  - Installing pydantic-core (2.20.1)\n",
      "  - Installing requests (2.32.3)\n",
      "  - Installing httpx (0.28.1)\n",
      "  - Installing jsonpointer (3.0.0)\n",
      "  - Installing orjson (3.10.14)\n",
      "  - Installing pydantic (2.8.2)\n",
      "  - Installing requests-toolbelt (1.0.0)\n",
      "  - Installing asttokens (3.0.0)\n",
      "  - Installing colorama (0.4.6)\n",
      "  - Installing executing (2.1.0)\n",
      "  - Installing frozenlist (1.5.0)\n",
      "  - Installing jsonpatch (1.33)\n",
      "  - Installing langsmith (0.1.147)\n",
      "  - Installing multidict (6.1.0)\n",
      "  - Installing packaging (24.2)\n",
      "  - Installing parso (0.8.4)\n",
      "  - Installing propcache (0.2.1)\n",
      "  - Installing pure-eval (0.2.3)\n",
      "  - Installing pyyaml (6.0.2)\n",
      "  - Installing tenacity (8.5.0)\n",
      "  - Installing traitlets (5.14.3)\n",
      "  - Installing wcwidth (0.2.13)\n",
      "  - Installing aiohappyeyeballs (2.4.4)\n",
      "  - Installing aiosignal (1.3.2)\n",
      "  - Installing attrs (23.2.0)\n",
      "  - Installing decorator (5.1.1)\n",
      "  - Installing filelock (3.16.1)\n",
      "  - Installing fsspec (2024.9.0)\n",
      "  - Installing greenlet (3.1.1)\n",
      "  - Installing jedi (0.19.2)\n",
      "  - Installing langchain-core (0.2.43)\n",
      "  - Installing matplotlib-inline (0.1.7)\n",
      "  - Installing mypy-extensions (1.0.0)\n",
      "  - Installing prompt-toolkit (3.0.48)\n",
      "  - Installing pycparser (2.22)\n",
      "  - Installing pygments (2.19.1)\n",
      "  - Installing stack-data (0.6.3)\n",
      "  - Installing tqdm (4.67.1)\n",
      "  - Installing yarl (1.18.3)\n",
      "  - Installing aiohttp (3.11.11)\n",
      "  - Installing cffi (1.17.1)\n",
      "  - Installing comm (0.2.2)\n",
      "  - Installing distro (1.9.0)\n",
      "  - Installing hpack (4.0.0)\n",
      "  - Installing huggingface-hub (0.27.1)\n",
      "  - Installing hyperframe (6.0.1)\n",
      "  - Installing ipython (8.31.0)\n",
      "  - Installing jiter (0.8.2)\n",
      "  - Installing jupyterlab-widgets (3.0.13)\n",
      "  - Installing langchain-text-splitters (0.2.4)\n",
      "  - Installing markupsafe (3.0.2)\n",
      "  - Installing marshmallow (3.24.2)\n",
      "  - Installing mdurl (0.1.2)\n",
      "  - Installing mpmath (1.3.0)\n",
      "  - Installing numpy (1.26.4)\n",
      "  - Installing outcome (1.3.0.post0)\n",
      "  - Installing regex (2024.11.6)\n",
      "  - Installing six (1.17.0)\n",
      "  - Installing smmap (5.0.2)\n",
      "  - Installing sortedcontainers (2.4.0)\n",
      "  - Installing sqlalchemy (2.0.36)\n",
      "  - Installing typing-inspect (0.9.0)\n",
      "  - Installing widgetsnbextension (4.0.13)\n",
      "  - Installing bcrypt (4.0.1)\n",
      "  - Installing cachetools (5.5.0)\n",
      "  - Installing click (8.1.3)\n",
      "  - Installing cryptography (44.0.0)\n",
      "  - Installing dataclasses-json (0.6.7)\n",
      "  - Installing dill (0.3.8)\n",
      "  - Installing distlib (0.3.9)\n",
      "  - Installing gitdb (4.0.12)\n",
      "  - Installing grpcio (1.69.0)\n",
      "  - Installing h2 (4.1.0)\n",
      "  - Installing httptools (0.6.4)\n",
      "  - Installing iniconfig (2.0.0)\n",
      "  - Installing ipywidgets (8.1.5)\n",
      "  - Installing jinja2 (3.1.5)\n",
      "  - Installing joblib (1.4.2)\n",
      "  - Installing langchain (0.2.17)\n",
      "  - Installing mako (1.3.8)\n",
      "  - Installing markdown-it-py (3.0.0)\n",
      "  - Installing networkx (3.4.2)\n",
      "  - Installing openai (1.59.5)\n",
      "  - Installing platformdirs (4.3.6)\n",
      "  - Installing pluggy (1.5.0)\n",
      "  - Installing protobuf (5.29.3)\n",
      "  - Installing pysocks (1.7.1)\n",
      "  - Installing python-dateutil (2.9.0.post0)\n",
      "  - Installing python-dotenv (1.0.1)\n",
      "  - Installing pytz (2024.2)\n",
      "  - Installing pywin32 (308)\n",
      "  - Installing rapidfuzz (3.11.0)\n",
      "  - Installing safetensors (0.5.2)\n",
      "  - Installing scipy (1.15.0)\n",
      "  - Installing setuptools (75.8.0)\n",
      "  - Installing starlette (0.36.3)\n",
      "  - Installing sympy (1.13.1)\n",
      "  - Installing threadpoolctl (3.5.0)\n",
      "  - Installing tiktoken (0.7.0)\n",
      "  - Installing tokenizers (0.21.0)\n",
      "  - Installing trio (0.28.0)\n",
      "  - Installing tzdata (2024.2)\n",
      "  - Installing watchfiles (1.0.3)\n",
      "  - Installing websockets (14.1)\n",
      "  - Installing wsproto (1.2.0)\n",
      "  - Installing alembic (1.8.1)\n",
      "  - Installing cfgv (3.4.0)\n",
      "  - Installing cloudpickle (2.2.1)\n",
      "  - Installing dnspython (2.7.0)\n",
      "  - Installing docker (7.1.0)\n",
      "  - Installing fastapi (0.110.0)\n",
      "  - Installing gitpython (3.1.44)\n",
      "  - Installing grpcio-tools (1.69.0)\n",
      "  - Installing identify (2.6.5)\n",
      "  - Installing ipinfo (5.1.1)\n",
      "  - Installing langchain-community (0.2.19)\n",
      "  - Installing langchain-openai (0.1.25)\n",
      "  - Installing levenshtein (0.25.1)\n",
      "  - Installing multiprocess (0.70.16)\n",
      "  - Installing nodeenv (1.9.1)\n",
      "  - Installing pandas (2.2.3)\n",
      "  - Installing passlib (1.7.4)\n",
      "  - Installing pastel (0.2.1)\n",
      "  - Installing pillow (11.1.0)\n",
      "  - Installing portalocker (2.10.1)\n",
      "  - Installing psutil (6.1.1)\n",
      "  - Installing pyarrow (18.1.0)\n",
      "  - Installing pydantic-settings (2.7.1)\n",
      "  - Installing pyjwt (2.7.0)\n",
      "  - Installing pymysql (1.1.1)\n",
      "  - Installing pytest (8.3.4)\n",
      "  - Installing python-multipart (0.0.20)\n",
      "  - Installing questionary (2.1.0)\n",
      "  - Installing rich (13.9.4)\n",
      "  - Installing scikit-learn (1.6.0)\n",
      "  - Installing secure (0.3.0)\n",
      "  - Installing soupsieve (2.6)\n",
      "  - Installing sqlalchemy-utils (0.41.2)\n",
      "  - Installing sqlmodel (0.0.18)\n",
      "  - Installing torch (2.5.1)\n",
      "  - Installing transformers (4.47.1)\n",
      "  - Installing trio-websocket (0.11.1)\n",
      "  - Installing uuid7 (0.1.0)\n",
      "  - Installing uvicorn (0.30.6)\n",
      "  - Installing virtualenv (20.28.1)\n",
      "  - Installing websocket-client (1.8.0)\n",
      "  - Installing win32-setctime (1.2.0)\n",
      "  - Installing xxhash (3.5.0)\n",
      "  - Installing beautifulsoup4 (4.12.3)\n",
      "  - Installing chromedriver-autoinstaller (0.6.4)\n",
      "  - Installing datasets (3.2.0)\n",
      "  - Installing fake-useragent (1.5.1)\n",
      "  - Installing html2text (2024.2.26)\n",
      "  - Installing jmespath (1.0.1)\n",
      "  - Installing loguru (0.7.3)\n",
      "  - Installing opik (0.2.2)\n",
      "  - Installing poethepoet (0.29.0)\n",
      "  - Installing pre-commit (3.8.0)\n",
      "  - Installing pymongo (4.10.1)\n",
      "  - Installing qdrant-client (1.12.2)\n",
      "  - Installing ruff (0.4.10)\n",
      "  - Installing selenium (4.27.1)\n",
      "  - Installing sentence-transformers (3.3.1)\n",
      "  - Installing webdriver-manager (4.0.2)\n",
      "  - Installing zenml (0.67.0)\n",
      "\n",
      "Writing lock file\n",
      "\n",
      "Installing the current project: llm-engineering (0.1.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: The current project could not be installed: Readme path `c:\\Users\\asrah\\Desktop\\llm_twin\\README.md` does not exist.\n",
      "If you do not want to install the current project use --no-root.\n",
      "If you want to use Poetry only for dependency management but not for packaging, you can disable package mode by setting package-mode = false in your pyproject.toml file.\n",
      "If you did intend to install the current project, you may need to set `packages` in your pyproject.toml file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!poetry install --without aws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-commit installed at .git\\hooks\\pre-commit\n"
     ]
    }
   ],
   "source": [
    "!poetry run pre-commit install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Optional\n",
    "from pydantic import UUID4, Field\n",
    "from pydantic_settings import BaseSettings, SettingsConfigDict\n",
    "from zenml.client import Client\n",
    "from zenml.exceptions import EntityExistsError\n",
    "from abc import abstractmethod\n",
    "from tempfile import mkdtemp\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import time\n",
    "from typing import Dict, List\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from loguru import logger\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "from typing_extensions import Annotated\n",
    "from zenml import get_step_context, step\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import ConnectionFailure\n",
    "import uuid\n",
    "from typing import Generic, Type, TypeVar\n",
    "from pydantic import UUID4, BaseModel, Field\n",
    "from pymongo import errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-08 16:08:53.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__new__\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mConnection to MongoDB with URI successful: mongodb://llm_engineering:llm_engineering@127.0.0.1:27017\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class MongoDatabaseConnector:\n",
    "    _instance: MongoClient | None = None\n",
    "\n",
    "    def __new__(cls, *args, **kwargs) -> MongoClient:\n",
    "        if cls._instance is None:\n",
    "            try:\n",
    "                cls._instance = MongoClient(settings.DATABASE_HOST)\n",
    "            except ConnectionFailure as e:\n",
    "                logger.error(f\"Couldn't connect to the database: {e!s}\")\n",
    "\n",
    "                raise\n",
    "\n",
    "        logger.info(f\"Connection to MongoDB with URI successful: {settings.DATABASE_HOST}\")\n",
    "\n",
    "        return cls._instance\n",
    "\n",
    "\n",
    "connection = MongoDatabaseConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "\n",
    "class DataCategory(StrEnum):\n",
    "    PROMPT = \"prompt\"\n",
    "    QUERIES = \"queries\"\n",
    "\n",
    "    INSTRUCT_DATASET_SAMPLES = \"instruct_dataset_samples\"\n",
    "    INSTRUCT_DATASET = \"instruct_dataset\"\n",
    "    PREFERENCE_DATASET_SAMPLES = \"preference_dataset_samples\"\n",
    "    PREFERENCE_DATASET = \"preference_dataset\"\n",
    "\n",
    "    POSTS = \"posts\"\n",
    "    ARTICLES = \"articles\"\n",
    "    REPOSITORIES = \"repositories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-08 16:09:12.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_settings\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mLoading settings from the ZenML secret store.\u001b[0m\n",
      "\u001b[32m2025-01-08 16:09:12.940\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_settings\u001b[0m:\u001b[36m99\u001b[0m - \u001b[33m\u001b[1mFailed to load settings from the ZenML secret store. Defaulting to loading the settings from the '.env' file.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class Settings(BaseSettings):\n",
    "    model_config = SettingsConfigDict(env_file=\".env\", env_file_encoding=\"utf-8\")\n",
    "\n",
    "    # --- Required settings even when working locally. ---\n",
    "\n",
    "    # OpenAI API\n",
    "    OPENAI_MODEL_ID: str = \"gpt-4o-mini\"\n",
    "    OPENAI_API_KEY: str | None = None\n",
    "\n",
    "    # Huggingface API\n",
    "    HUGGINGFACE_ACCESS_TOKEN: str | None = None\n",
    "\n",
    "    # Comet ML (during training)\n",
    "    COMET_API_KEY: str | None = None\n",
    "    COMET_PROJECT: str = \"twin\"\n",
    "\n",
    "    # --- Required settings when deploying the code. ---\n",
    "    # --- Otherwise, default values values work fine. ---\n",
    "\n",
    "    # MongoDB database\n",
    "    DATABASE_HOST: str = \"mongodb://llm_engineering:llm_engineering@127.0.0.1:27017\"\n",
    "    DATABASE_NAME: str = \"twin\"\n",
    "\n",
    "    # Qdrant vector database\n",
    "    USE_QDRANT_CLOUD: bool = False\n",
    "    QDRANT_DATABASE_HOST: str = \"localhost\"\n",
    "    QDRANT_DATABASE_PORT: int = 6333\n",
    "    QDRANT_CLOUD_URL: str = \"str\"\n",
    "    QDRANT_APIKEY: str | None = None\n",
    "\n",
    "    # AWS Authentication\n",
    "    AWS_REGION: str = \"eu-central-1\"\n",
    "    AWS_ACCESS_KEY: str | None = None\n",
    "    AWS_SECRET_KEY: str | None = None\n",
    "    AWS_ARN_ROLE: str | None = None\n",
    "\n",
    "    # --- Optional settings used to tweak the code. ---\n",
    "\n",
    "    # AWS SageMaker\n",
    "    HF_MODEL_ID: str = \"mlabonne/TwinLlama-3.1-8B-DPO\"\n",
    "    GPU_INSTANCE_TYPE: str = \"ml.g5.2xlarge\"\n",
    "    SM_NUM_GPUS: int = 1\n",
    "    MAX_INPUT_LENGTH: int = 2048\n",
    "    MAX_TOTAL_TOKENS: int = 4096\n",
    "    MAX_BATCH_TOTAL_TOKENS: int = 4096\n",
    "    COPIES: int = 1  # Number of replicas\n",
    "    GPUS: int = 1  # Number of GPUs\n",
    "    CPUS: int = 2  # Number of CPU cores\n",
    "\n",
    "    SAGEMAKER_ENDPOINT_CONFIG_INFERENCE: str = \"twin\"\n",
    "    SAGEMAKER_ENDPOINT_INFERENCE: str = \"twin\"\n",
    "    TEMPERATURE_INFERENCE: float = 0.01\n",
    "    TOP_P_INFERENCE: float = 0.9\n",
    "    MAX_NEW_TOKENS_INFERENCE: int = 150\n",
    "\n",
    "    # RAG\n",
    "    TEXT_EMBEDDING_MODEL_ID: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    RERANKING_CROSS_ENCODER_MODEL_ID: str = \"cross-encoder/ms-marco-MiniLM-L-4-v2\"\n",
    "    RAG_MODEL_DEVICE: str = \"cpu\"\n",
    "\n",
    "    # LinkedIn Credentials\n",
    "    LINKEDIN_USERNAME: str | None = None\n",
    "    LINKEDIN_PASSWORD: str | None = None\n",
    "\n",
    "    @property\n",
    "    def OPENAI_MAX_TOKEN_WINDOW(self) -> int:\n",
    "        official_max_token_window = {\n",
    "            \"gpt-3.5-turbo\": 16385,\n",
    "            \"gpt-4-turbo\": 128000,\n",
    "            \"gpt-4o\": 128000,\n",
    "            \"gpt-4o-mini\": 128000,\n",
    "        }.get(self.OPENAI_MODEL_ID, 128000)\n",
    "\n",
    "        max_token_window = int(official_max_token_window * 0.90)\n",
    "\n",
    "        return max_token_window\n",
    "\n",
    "    @classmethod\n",
    "    def load_settings(cls) -> \"Settings\":\n",
    "        \"\"\"\n",
    "        Tries to load the settings from the ZenML secret store. If the secret does not exist, it initializes the settings from the .env file and default values.\n",
    "\n",
    "        Returns:\n",
    "            Settings: The initialized settings object.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Loading settings from the ZenML secret store.\")\n",
    "\n",
    "            settings_secrets = Client().get_secret(\"settings\")\n",
    "            settings = Settings(**settings_secrets.secret_values)\n",
    "        except (RuntimeError, KeyError):\n",
    "            logger.warning(\n",
    "                \"Failed to load settings from the ZenML secret store. Defaulting to loading the settings from the '.env' file.\"\n",
    "            )\n",
    "            settings = Settings()\n",
    "\n",
    "        return settings\n",
    "\n",
    "    def export(self) -> None:\n",
    "        \"\"\"\n",
    "        Exports the settings to the ZenML secret store.\n",
    "        \"\"\"\n",
    "\n",
    "        env_vars = settings.model_dump()\n",
    "        for key, value in env_vars.items():\n",
    "            env_vars[key] = str(value)\n",
    "\n",
    "        client = Client()\n",
    "\n",
    "        try:\n",
    "            client.create_secret(name=\"settings\", values=env_vars)\n",
    "        except EntityExistsError:\n",
    "            logger.warning(\n",
    "                \"Secret 'scope' already exists. Delete it manually by running 'zenml secret delete settings', before trying to recreate it.\"\n",
    "            )\n",
    "\n",
    "\n",
    "settings = Settings.load_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMTwinException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ImproperlyConfigured(LLMTwinException):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mChromedriver is already installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if the current version of chromedriver exists\n",
    "# and if it doesn't exist, download it automatically,\n",
    "# then add chromedriver to path\n",
    "chromedriver_autoinstaller.install()\n",
    "class BaseCrawler(ABC):\n",
    "    model: type[NoSQLBaseDocument]\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract(self, link: str, **kwargs) -> None: ...\n",
    "\n",
    "\n",
    "class BaseSeleniumCrawler(BaseCrawler, ABC):\n",
    "    def __init__(self, scroll_limit: int = 5) -> None:\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--log-level=3\")\n",
    "        options.add_argument(\"--disable-popup-blocking\")\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-extensions\")\n",
    "        options.add_argument(\"--disable-background-networking\")\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n",
    "        options.add_argument(f\"--data-path={mkdtemp()}\")\n",
    "        options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n",
    "        options.add_argument(\"--remote-debugging-port=9226\")\n",
    "\n",
    "        self.set_extra_driver_options(options)\n",
    "\n",
    "        self.scroll_limit = scroll_limit\n",
    "        self.driver = webdriver.Chrome(\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "    def set_extra_driver_options(self, options: Options) -> None:\n",
    "        pass\n",
    "\n",
    "    def login(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n",
    "        current_scroll = 0\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height or (self.scroll_limit and current_scroll >= self.scroll_limit):\n",
    "                break\n",
    "            last_height = new_height\n",
    "            current_scroll += 1\n",
    "\n",
    "class CustomArticleCrawler(BaseCrawler):\n",
    "    model = ArticleDocument\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def extract(self, link: str, **kwargs) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Article already exists in the database: {link}\")\n",
    "\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scrapping article: {link}\")\n",
    "\n",
    "        loader = AsyncHtmlLoader([link])\n",
    "        docs = loader.load()\n",
    "\n",
    "        html2text = Html2TextTransformer()\n",
    "        docs_transformed = html2text.transform_documents(docs)\n",
    "        doc_transformed = docs_transformed[0]\n",
    "\n",
    "        content = {\n",
    "            \"Title\": doc_transformed.metadata.get(\"title\"),\n",
    "            \"Subtitle\": doc_transformed.metadata.get(\"description\"),\n",
    "            \"Content\": doc_transformed.page_content,\n",
    "            \"language\": doc_transformed.metadata.get(\"language\"),\n",
    "        }\n",
    "\n",
    "        parsed_url = urlparse(link)\n",
    "        platform = parsed_url.netloc\n",
    "\n",
    "        user = kwargs[\"user\"]\n",
    "        instance = self.model(\n",
    "            content=content,\n",
    "            link=link,\n",
    "            platform=platform,\n",
    "            author_id=user.id,\n",
    "            author_full_name=user.full_name,\n",
    "        )\n",
    "        instance.save()\n",
    "\n",
    "        logger.info(f\"Finished scrapping custom article: {link}\")\n",
    "\n",
    "class GithubCrawler(BaseCrawler):\n",
    "    model = RepositoryDocument\n",
    "\n",
    "    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n",
    "        super().__init__()\n",
    "        self._ignore = ignore\n",
    "\n",
    "    def extract(self, link: str, **kwargs) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Repository already exists in the database: {link}\")\n",
    "\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scrapping GitHub repository: {link}\")\n",
    "\n",
    "        repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "        local_temp = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            os.chdir(local_temp)\n",
    "            subprocess.run([\"git\", \"clone\", link])\n",
    "\n",
    "            repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])  # noqa: PTH118\n",
    "\n",
    "            tree = {}\n",
    "            for root, _, files in os.walk(repo_path):\n",
    "                dir = root.replace(repo_path, \"\").lstrip(\"/\")\n",
    "                if dir.startswith(self._ignore):\n",
    "                    continue\n",
    "\n",
    "                for file in files:\n",
    "                    if file.endswith(self._ignore):\n",
    "                        continue\n",
    "                    file_path = os.path.join(dir, file)  # noqa: PTH118\n",
    "                    with open(os.path.join(root, file), \"r\", errors=\"ignore\") as f:  # noqa: PTH123, PTH118\n",
    "                        tree[file_path] = f.read().replace(\" \", \"\")\n",
    "\n",
    "            user = kwargs[\"user\"]\n",
    "            instance = self.model(\n",
    "                content=tree,\n",
    "                name=repo_name,\n",
    "                link=link,\n",
    "                platform=\"github\",\n",
    "                author_id=user.id,\n",
    "                author_full_name=user.full_name,\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception:\n",
    "            raise\n",
    "        finally:\n",
    "            shutil.rmtree(local_temp)\n",
    "\n",
    "        logger.info(f\"Finished scrapping GitHub repository: {link}\")\n",
    "\n",
    "class LinkedInCrawler(BaseSeleniumCrawler):\n",
    "    model = PostDocument\n",
    "\n",
    "    def __init__(self, scroll_limit: int = 5, is_deprecated: bool = True) -> None:\n",
    "        super().__init__(scroll_limit)\n",
    "\n",
    "        self._is_deprecated = is_deprecated\n",
    "\n",
    "    def set_extra_driver_options(self, options) -> None:\n",
    "        options.add_experimental_option(\"detach\", True)\n",
    "\n",
    "    def login(self) -> None:\n",
    "        if self._is_deprecated:\n",
    "            raise DeprecationWarning(\n",
    "                \"As LinkedIn has updated its security measures, the login() method is no longer supported.\"\n",
    "            )\n",
    "\n",
    "        self.driver.get(\"https://www.linkedin.com/login\")\n",
    "        if not settings.LINKEDIN_USERNAME or not settings.LINKEDIN_PASSWORD:\n",
    "            raise ImproperlyConfigured(\n",
    "                \"LinkedIn scraper requires the {LINKEDIN_USERNAME} and {LINKEDIN_PASSWORD} settings.\"\n",
    "            )\n",
    "\n",
    "        self.driver.find_element(By.ID, \"username\").send_keys(settings.LINKEDIN_USERNAME)\n",
    "        self.driver.find_element(By.ID, \"password\").send_keys(settings.LINKEDIN_PASSWORD)\n",
    "        self.driver.find_element(By.CSS_SELECTOR, \".login__form_action_container button\").click()\n",
    "\n",
    "    def extract(self, link: str, **kwargs) -> None:\n",
    "        if self._is_deprecated:\n",
    "            raise DeprecationWarning(\n",
    "                \"As LinkedIn has updated its feed structure, the extract() method is no longer supported.\"\n",
    "            )\n",
    "\n",
    "        if self.model.link is not None:\n",
    "            old_model = self.model.find(link=link)\n",
    "            if old_model is not None:\n",
    "                logger.info(f\"Post already exists in the database: {link}\")\n",
    "\n",
    "                return\n",
    "\n",
    "        logger.info(f\"Starting scrapping data for profile: {link}\")\n",
    "\n",
    "        self.login()\n",
    "\n",
    "        soup = self._get_page_content(link)\n",
    "\n",
    "        data = {  # noqa\n",
    "            \"Name\": self._scrape_section(soup, \"h1\", class_=\"text-heading-xlarge\"),\n",
    "            \"About\": self._scrape_section(soup, \"div\", class_=\"display-flex ph5 pv3\"),\n",
    "            \"Main Page\": self._scrape_section(soup, \"div\", {\"id\": \"main-content\"}),\n",
    "            \"Experience\": self._scrape_experience(link),\n",
    "            \"Education\": self._scrape_education(link),\n",
    "        }\n",
    "\n",
    "        self.driver.get(link)\n",
    "        time.sleep(5)\n",
    "        button = self.driver.find_element(\n",
    "            By.CSS_SELECTOR, \".app-aware-link.profile-creator-shared-content-view__footer-action\"\n",
    "        )\n",
    "        button.click()\n",
    "\n",
    "        # Scrolling and scraping posts\n",
    "        self.scroll_page()\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        post_elements = soup.find_all(\n",
    "            \"div\",\n",
    "            class_=\"update-components-text relative update-components-update-v2__commentary\",\n",
    "        )\n",
    "        buttons = soup.find_all(\"button\", class_=\"update-components-image__image-link\")\n",
    "        post_images = self._extract_image_urls(buttons)\n",
    "\n",
    "        posts = self._extract_posts(post_elements, post_images)\n",
    "        logger.info(f\"Found {len(posts)} posts for profile: {link}\")\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        user = kwargs[\"user\"]\n",
    "        self.model.bulk_insert(\n",
    "            [\n",
    "                PostDocument(platform=\"linkedin\", content=post, author_id=user.id, author_full_name=user.full_name)\n",
    "                for post in posts\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Finished scrapping data for profile: {link}\")\n",
    "\n",
    "    def _scrape_section(self, soup: BeautifulSoup, *args, **kwargs) -> str:\n",
    "        \"\"\"Scrape a specific section of the LinkedIn profile.\"\"\"\n",
    "        # Example: Scrape the 'About' section\n",
    "\n",
    "        parent_div = soup.find(*args, **kwargs)\n",
    "\n",
    "        return parent_div.get_text(strip=True) if parent_div else \"\"\n",
    "\n",
    "    def _extract_image_urls(self, buttons: List[Tag]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Extracts image URLs from button elements.\n",
    "\n",
    "        Args:\n",
    "            buttons (List[Tag]): A list of BeautifulSoup Tag objects representing buttons.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, str]: A dictionary mapping post indexes to image URLs.\n",
    "        \"\"\"\n",
    "\n",
    "        post_images = {}\n",
    "        for i, button in enumerate(buttons):\n",
    "            img_tag = button.find(\"img\")\n",
    "            if img_tag and \"src\" in img_tag.attrs:\n",
    "                post_images[f\"Post_{i}\"] = img_tag[\"src\"]\n",
    "            else:\n",
    "                logger.warning(\"No image found in this button\")\n",
    "        return post_images\n",
    "\n",
    "    def _get_page_content(self, url: str) -> BeautifulSoup:\n",
    "        \"\"\"Retrieve the page content of a given URL.\"\"\"\n",
    "\n",
    "        self.driver.get(url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        return BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "\n",
    "    def _extract_posts(self, post_elements: List[Tag], post_images: Dict[str, str]) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extracts post texts and combines them with their respective images.\n",
    "\n",
    "        Args:\n",
    "            post_elements (List[Tag]): A list of BeautifulSoup Tag objects representing post elements.\n",
    "            post_images (Dict[str, str]): A dictionary containing image URLs mapped by post index.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, str]]: A dictionary containing post data with text and optional image URL.\n",
    "        \"\"\"\n",
    "\n",
    "        posts_data = {}\n",
    "        for i, post_element in enumerate(post_elements):\n",
    "            post_text = post_element.get_text(strip=True, separator=\"\\n\")\n",
    "            post_data = {\"text\": post_text}\n",
    "            if f\"Post_{i}\" in post_images:\n",
    "                post_data[\"image\"] = post_images[f\"Post_{i}\"]\n",
    "            posts_data[f\"Post_{i}\"] = post_data\n",
    "\n",
    "        return posts_data\n",
    "\n",
    "    def _scrape_experience(self, profile_url: str) -> str:\n",
    "        \"\"\"Scrapes the Experience section of the LinkedIn profile.\"\"\"\n",
    "\n",
    "        self.driver.get(profile_url + \"/details/experience/\")\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        experience_content = soup.find(\"section\", {\"id\": \"experience-section\"})\n",
    "\n",
    "        return experience_content.get_text(strip=True) if experience_content else \"\"\n",
    "\n",
    "    def _scrape_education(self, profile_url: str) -> str:\n",
    "        self.driver.get(profile_url + \"/details/education/\")\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        education_content = soup.find(\"section\", {\"id\": \"education-section\"})\n",
    "\n",
    "        return education_content.get_text(strip=True) if education_content else \"\"\n",
    "class MediumCrawler(BaseSeleniumCrawler):\n",
    "    model = ArticleDocument\n",
    "\n",
    "    def set_extra_driver_options(self, options) -> None:\n",
    "        options.add_argument(r\"--profile-directory=Profile 2\")\n",
    "\n",
    "    def extract(self, link: str, **kwargs) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Article already exists in the database: {link}\")\n",
    "\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scrapping Medium article: {link}\")\n",
    "\n",
    "        self.driver.get(link)\n",
    "        self.scroll_page()\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n",
    "        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n",
    "\n",
    "        data = {\n",
    "            \"Title\": title[0].string if title else None,\n",
    "            \"Subtitle\": subtitle[0].string if subtitle else None,\n",
    "            \"Content\": soup.get_text(),\n",
    "        }\n",
    "\n",
    "        self.driver.close()\n",
    "\n",
    "        user = kwargs[\"user\"]\n",
    "        instance = self.model(\n",
    "            platform=\"medium\",\n",
    "            content=data,\n",
    "            link=link,\n",
    "            author_id=user.id,\n",
    "            author_full_name=user.full_name,\n",
    "        )\n",
    "        instance.save()\n",
    "\n",
    "        logger.info(f\"Successfully scraped and saved article: {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "_database = connection.get_database(settings.DATABASE_NAME)\n",
    "\n",
    "\n",
    "T = TypeVar(\"T\", bound=\"NoSQLBaseDocument\")\n",
    "\n",
    "\n",
    "class NoSQLBaseDocument(BaseModel, Generic[T], ABC):\n",
    "    id: UUID4 = Field(default_factory=uuid.uuid4)\n",
    "\n",
    "    def __eq__(self, value: object) -> bool:\n",
    "        if not isinstance(value, self.__class__):\n",
    "            return False\n",
    "\n",
    "        return self.id == value.id\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash(self.id)\n",
    "\n",
    "    @classmethod\n",
    "    def from_mongo(cls: Type[T], data: dict) -> T:\n",
    "        \"\"\"Convert \"_id\" (str object) into \"id\" (UUID object).\"\"\"\n",
    "\n",
    "        if not data:\n",
    "            raise ValueError(\"Data is empty.\")\n",
    "\n",
    "        id = data.pop(\"_id\")\n",
    "\n",
    "        return cls(**dict(data, id=id))\n",
    "\n",
    "    def to_mongo(self: T, **kwargs) -> dict:\n",
    "        \"\"\"Convert \"id\" (UUID object) into \"_id\" (str object).\"\"\"\n",
    "        exclude_unset = kwargs.pop(\"exclude_unset\", False)\n",
    "        by_alias = kwargs.pop(\"by_alias\", True)\n",
    "\n",
    "        parsed = self.model_dump(exclude_unset=exclude_unset, by_alias=by_alias, **kwargs)\n",
    "\n",
    "        if \"_id\" not in parsed and \"id\" in parsed:\n",
    "            parsed[\"_id\"] = str(parsed.pop(\"id\"))\n",
    "\n",
    "        for key, value in parsed.items():\n",
    "            if isinstance(value, uuid.UUID):\n",
    "                parsed[key] = str(value)\n",
    "\n",
    "        return parsed\n",
    "\n",
    "    def model_dump(self: T, **kwargs) -> dict:\n",
    "        dict_ = super().model_dump(**kwargs)\n",
    "\n",
    "        for key, value in dict_.items():\n",
    "            if isinstance(value, uuid.UUID):\n",
    "                dict_[key] = str(value)\n",
    "\n",
    "        return dict_\n",
    "\n",
    "    def save(self: T, **kwargs) -> T | None:\n",
    "        collection = _database[self.get_collection_name()]\n",
    "        try:\n",
    "            collection.insert_one(self.to_mongo(**kwargs))\n",
    "\n",
    "            return self\n",
    "        except errors.WriteError:\n",
    "            logger.exception(\"Failed to insert document.\")\n",
    "\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def get_or_create(cls: Type[T], **filter_options) -> T:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instance = collection.find_one(filter_options)\n",
    "            if instance:\n",
    "                return cls.from_mongo(instance)\n",
    "\n",
    "            new_instance = cls(**filter_options)\n",
    "            new_instance = new_instance.save()\n",
    "\n",
    "            return new_instance\n",
    "        except errors.OperationFailure:\n",
    "            logger.exception(f\"Failed to retrieve document with filter options: {filter_options}\")\n",
    "\n",
    "            raise\n",
    "\n",
    "    @classmethod\n",
    "    def bulk_insert(cls: Type[T], documents: list[T], **kwargs) -> bool:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            collection.insert_many(doc.to_mongo(**kwargs) for doc in documents)\n",
    "\n",
    "            return True\n",
    "        except (errors.WriteError, errors.BulkWriteError):\n",
    "            logger.error(f\"Failed to insert documents of type {cls.__name__}\")\n",
    "\n",
    "            return False\n",
    "\n",
    "    @classmethod\n",
    "    def find(cls: Type[T], **filter_options) -> T | None:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instance = collection.find_one(filter_options)\n",
    "            if instance:\n",
    "                return cls.from_mongo(instance)\n",
    "\n",
    "            return None\n",
    "        except errors.OperationFailure:\n",
    "            logger.error(\"Failed to retrieve document\")\n",
    "\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def bulk_find(cls: Type[T], **filter_options) -> list[T]:\n",
    "        collection = _database[cls.get_collection_name()]\n",
    "        try:\n",
    "            instances = collection.find(filter_options)\n",
    "            return [document for instance in instances if (document := cls.from_mongo(instance)) is not None]\n",
    "        except errors.OperationFailure:\n",
    "            logger.error(\"Failed to retrieve documents\")\n",
    "\n",
    "            return []\n",
    "\n",
    "    @classmethod\n",
    "    def get_collection_name(cls: Type[T]) -> str:\n",
    "        if not hasattr(cls, \"Settings\") or not hasattr(cls.Settings, \"name\"):\n",
    "            raise ImproperlyConfigured(\n",
    "                \"Document should define an Settings configuration class with the name of the collection.\"\n",
    "            )\n",
    "\n",
    "        return cls.Settings.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserDocument(NoSQLBaseDocument):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "\n",
    "    class Settings:\n",
    "        name = \"users\"\n",
    "\n",
    "    @property\n",
    "    def full_name(self):\n",
    "        return f\"{self.first_name} {self.last_name}\"\n",
    "\n",
    "\n",
    "class Document(NoSQLBaseDocument, ABC):\n",
    "    content: dict\n",
    "    platform: str\n",
    "    author_id: UUID4 = Field(alias=\"author_id\")\n",
    "    author_full_name: str = Field(alias=\"author_full_name\")\n",
    "\n",
    "\n",
    "class RepositoryDocument(Document):\n",
    "    name: str\n",
    "    link: str\n",
    "\n",
    "    class Settings:\n",
    "        name = DataCategory.REPOSITORIES\n",
    "\n",
    "\n",
    "class PostDocument(Document):\n",
    "    image: Optional[str] = None\n",
    "    link: str | None = None\n",
    "\n",
    "    class Settings:\n",
    "        name = DataCategory.POSTS\n",
    "\n",
    "\n",
    "class ArticleDocument(Document):\n",
    "    link: str\n",
    "\n",
    "    class Settings:\n",
    "        name = DataCategory.ARTICLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlerDispatcher:\n",
    "    def __init__(self) -> None:\n",
    "        self._crawlers = {}\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls) -> \"CrawlerDispatcher\":\n",
    "        dispatcher = cls()\n",
    "\n",
    "        return dispatcher\n",
    "\n",
    "    def register_medium(self) -> \"CrawlerDispatcher\":\n",
    "        self.register(\"https://medium.com\", MediumCrawler)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def register_linkedin(self) -> \"CrawlerDispatcher\":\n",
    "        self.register(\"https://linkedin.com\", LinkedInCrawler)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def register_github(self) -> \"CrawlerDispatcher\":\n",
    "        self.register(\"https://github.com\", GithubCrawler)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def register(self, domain: str, crawler: type[BaseCrawler]) -> None:\n",
    "        parsed_domain = urlparse(domain)\n",
    "        domain = parsed_domain.netloc\n",
    "\n",
    "        self._crawlers[r\"https://(www\\.)?{}/*\".format(re.escape(domain))] = crawler\n",
    "\n",
    "    def get_crawler(self, url: str) -> BaseCrawler:\n",
    "        for pattern, crawler in self._crawlers.items():\n",
    "            if re.match(pattern, url):\n",
    "                return crawler()\n",
    "        else:\n",
    "            logger.warning(f\"No crawler found for {url}. Defaulting to CustomArticleCrawler.\")\n",
    "\n",
    "            return CustomArticleCrawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def crawl_links(user: UserDocument, links: list[str]) -> Annotated[list[str], \"crawled_links\"]:\n",
    "    dispatcher = CrawlerDispatcher.build().register_linkedin().register_medium().register_github()\n",
    "\n",
    "    logger.info(f\"Starting to crawl {len(links)} link(s).\")\n",
    "\n",
    "    metadata = {}\n",
    "    successfull_crawls = 0\n",
    "    for link in tqdm(links):\n",
    "        successfull_crawl, crawled_domain = _crawl_link(dispatcher, link, user)\n",
    "        successfull_crawls += successfull_crawl\n",
    "\n",
    "        metadata = _add_to_metadata(metadata, crawled_domain, successfull_crawl)\n",
    "\n",
    "    step_context = get_step_context()\n",
    "    step_context.add_output_metadata(output_name=\"crawled_links\", metadata=metadata)\n",
    "\n",
    "    logger.info(f\"Successfully crawled {successfull_crawls} / {len(links)} links.\")\n",
    "\n",
    "    return links\n",
    "\n",
    "\n",
    "def _crawl_link(dispatcher: CrawlerDispatcher, link: str, user: UserDocument) -> tuple[bool, str]:\n",
    "    crawler = dispatcher.get_crawler(link)\n",
    "    crawler_domain = urlparse(link).netloc\n",
    "\n",
    "    try:\n",
    "        crawler.extract(link=link, user=user)\n",
    "\n",
    "        return (True, crawler_domain)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while crowling: {e!s}\")\n",
    "\n",
    "        return (False, crawler_domain)\n",
    "\n",
    "\n",
    "def _add_to_metadata(metadata: dict, domain: str, successfull_crawl: bool) -> dict:\n",
    "    if domain not in metadata:\n",
    "        metadata[domain] = {}\n",
    "    metadata[domain][\"successful\"] = metadata.get(domain, {}).get(\"successful\", 0) + successfull_crawl\n",
    "    metadata[domain][\"total\"] = metadata.get(domain, {}).get(\"total\", 0) + 1\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def digital_data_etl(user_full_name: str, links: list[str]) -> str:\n",
    "    user = get_or_create_user(user_full_name)\n",
    "    last_step = crawl_links(user=user, links=links)\n",
    "\n",
    "    return last_step.invocation_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
